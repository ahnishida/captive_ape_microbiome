{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script downloads metagenomic datasets useds by Pasoli et al. 2019 for their lab repository. This fastas unzipped take up roughly 2.3T. gyrb reads are filtered out using a blastn search by a fasta containing ASVs from the amplicon datasets and reads from the gyrb gtdbtk reference fasta. Reads are output into an ASV table where the columns represent the sequences and the rows represent datasets, and the values represent the number of samples that particular read was found. This enables the metagenomic data to be merged with the amplicon data. Although it should be noted that the read numbers between the two types of datasets are incomparable, and should be used to indicate presene/absence in various sample types.\n",
    "\n",
    "Inputs: list of metagenomic datasets, Bt-ASV fasta\n",
    "\n",
    "Output: ASV table of metagenomic reads to be merged with amplicon datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/AHN/captive_ape_microbiome/scripts/gyrb/processing/metagenomic_samples'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio import Seq\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of inputs/outpus\n",
    "due to the size of the data file only the results files will be permanently saved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#root_dir = '/Volumes/APE_MacPro_External_2/alex/captive_ape_microbiome'\n",
    "\n",
    "root_dir = '/Volumes/AHN/captive_ape_microbiome/'\n",
    "os.chdir(root_dir)\n",
    "#analysis_dir = root_dir + '/results/gyrb_metagenomic'\n",
    "analysis_dir = 'results/gyrb/processing/metagenomic_samples/'\n",
    "metagenomic_https_file = 'metadata_http_to_dnload.txt'\n",
    "metagenomic_samples_file = 'metadata_all_samples.txt'\n",
    "ASV_fasta =  'Bt_ASV_gtdbtk.fasta'\n",
    "\n",
    "os.chdir(analysis_dir)\n",
    "os.system('mkdir data') #\n",
    "os.system('mkdir sample_files')\n",
    "os.system('mkdir blastout')\n",
    "os.system('mkdir parse_blast')\n",
    "os.system('mkdir sseq_adj')\n",
    "os.system('mkdir final_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>https</th>\n",
       "      <th>filename</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dropbox.com/s/njbwzpazz4a1dy1/Asni...</td>\n",
       "      <td>AsnicarF_2017.tar.bz2</td>\n",
       "      <td>AsnicarF_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.dropbox.com/s/n6867hhmo12xnwf/Back...</td>\n",
       "      <td>BackhedF_2015.tar.bz2</td>\n",
       "      <td>BackhedF_2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dropbox.com/s/ac6cpq50pt5mqj6/Beng...</td>\n",
       "      <td>Bengtsson-PalmeJ_2015.tar.bz2</td>\n",
       "      <td>Bengtsson-PalmeJ_2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.dropbox.com/s/uj9cy9914l0cq3x/Brit...</td>\n",
       "      <td>BritoIL_2016.tar.bz2</td>\n",
       "      <td>BritoIL_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dropbox.com/s/dtxmz1apdgtf3d1/Cast...</td>\n",
       "      <td>Castro-NallarE_2015.tar.bz2</td>\n",
       "      <td>Castro-NallarE_2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               https  \\\n",
       "0  https://www.dropbox.com/s/njbwzpazz4a1dy1/Asni...   \n",
       "1  https://www.dropbox.com/s/n6867hhmo12xnwf/Back...   \n",
       "2  https://www.dropbox.com/s/ac6cpq50pt5mqj6/Beng...   \n",
       "3  https://www.dropbox.com/s/uj9cy9914l0cq3x/Brit...   \n",
       "4  https://www.dropbox.com/s/dtxmz1apdgtf3d1/Cast...   \n",
       "\n",
       "                        filename                 folder  \n",
       "0          AsnicarF_2017.tar.bz2          AsnicarF_2017  \n",
       "1          BackhedF_2015.tar.bz2          BackhedF_2015  \n",
       "2  Bengtsson-PalmeJ_2015.tar.bz2  Bengtsson-PalmeJ_2015  \n",
       "3           BritoIL_2016.tar.bz2           BritoIL_2016  \n",
       "4    Castro-NallarE_2015.tar.bz2    Castro-NallarE_2015  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list all metagenomic datasets\n",
    "metagenomic_datasets = pd.read_csv(metagenomic_https_file,header=None)\n",
    "metagenomic_datasets.columns = ['https']\n",
    "metagenomic_datasets['filename'] = metagenomic_datasets['https'].apply(lambda x: x.split('/')[-1])\n",
    "metagenomic_datasets['folder'] = metagenomic_datasets['filename'].apply(lambda x: x.split('.')[0])\n",
    "metagenomic_datasets.head()\n",
    "metagenomic_datasets.to_csv('metadata_datasets.txt',sep='\\t',index=False)\n",
    "metagenomic_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets already downloaded\n",
      "['AsnicarF_2017', 'BackhedF_2015', 'Bengtsson-PalmeJ_2015', 'BritoIL_2016', 'Castro-NallarE_2015', 'ChengpingW_2017', 'ChngKR_2016', 'CM_cf', 'CM_madagascar', 'CM_periimplantitis', 'CosteaPI_2017', 'DavidLA_2015', 'FengQ_2015', 'FerrettiP_2018', 'GeversD_2014', 'HanniganGD_2017', 'HeQ_2017', 'HMP_2012', 'IjazUZ_2017', 'KarlssonFH_2013', 'KosticAD_2015', 'LeChatelierE_2013', 'LiJ_2014', 'LiJ_2017', 'LiSS_2016', 'LiuW_2016', 'LomanNJ_2013', 'LoombaR_2017', 'LouisS_2016', 'NielsenHB_2014', 'Obregon-TitoAJ_2015', 'OhJ_2014', 'OlmMR_2017', 'QinJ_2012', 'QinN_2014', 'RampelliS_2015', 'RaymondF_2016', 'SchirmerM_2016', 'SmitsSA_2017', 'VatanenT_2016', 'VincentC_2016', 'VogtmannE_2016', 'WenC_2017', 'XieH_2016', 'YuJ_2015', 'ZeeviD_2015', 'ZellerG_2014']\n",
      "\n",
      "datasets left to download\n",
      "Empty DataFrame\n",
      "Columns: [https, filename, folder]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def dnload_file(http): \n",
    "    \"\"\"\n",
    "    downloads and expands metagenomic dataset from Passoli repo, generates lists of sample files in the data folder\n",
    "    \"\"\"\n",
    "    filename = http.split('/')[-1]\n",
    "    folder = filename.split('.')[0]\n",
    "    print(http,filename,folder)\n",
    "    \n",
    "    os.system(f'wget {http} --directory-prefix data/')\n",
    "    os.system(f'tar -vxf data/{filename} -C data/ ')\n",
    "    os.system(f'ls data/{folder}/*.fa > sample_files/{folder}_files.txt')  \n",
    "#dnload_file('https://www.dropbox.com/s/njbwzpazz4a1dy1/AsnicarF_2017.tar.bz2')\n",
    "\n",
    "\n",
    "#list datasets already downloaded\n",
    "datasets_downloaded = [f.split('.')[0] for f in os.listdir('data/') if f.endswith('bz2')]\n",
    "print('datasets already downloaded')\n",
    "print(datasets_downloaded)\n",
    "\n",
    "#determine datasets that need to be downloads\n",
    "datasets_to_do = metagenomic_datasets[~metagenomic_datasets['folder'].isin(datasets_downloaded)]\n",
    "print('\\ndatasets left to download')\n",
    "print(datasets_to_do)\n",
    "for http in datasets_to_do['https']:\n",
    "    print(http)\n",
    "    #dnload_file(http)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper scripts big to small for filtering gyrb reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AsnicarF_2017__MV_FEI1_t1Q14.fa', 'AsnicarF_2017__MV_FEI2_t1Q14.fa', 'AsnicarF_2017__MV_FEI3_t1Q14.fa', 'AsnicarF_2017__MV_FEI4_t1Q14.fa', 'AsnicarF_2017__MV_FEI4_t2Q15.fa', 'AsnicarF_2017__MV_FEI5_t1Q14.fa', 'AsnicarF_2017__MV_FEI5_t2Q14.fa', 'AsnicarF_2017__MV_FEI5_t3Q15.fa', 'AsnicarF_2017__MV_FEM1_t1Q14.fa', 'AsnicarF_2017__MV_FEM2_t1Q14.fa', 'AsnicarF_2017__MV_FEM3_t1Q14.fa', 'AsnicarF_2017__MV_FEM4_t1Q14.fa', 'AsnicarF_2017__MV_FEM4_t2Q15.fa', 'AsnicarF_2017__MV_FEM5_t1Q14.fa', 'AsnicarF_2017__MV_FEM5_t2Q14.fa', 'AsnicarF_2017__MV_FEM5_t3Q15.fa', 'AsnicarF_2017__MV_MIM2_t1M14.fa', 'AsnicarF_2017__MV_MIM3_t1M14.fa', 'AsnicarF_2017__MV_MIM4_t2F15.fa', 'AsnicarF_2017__MV_MIM5_t2M14.fa', 'AsnicarF_2017__MV_MIM5_t3F15.fa']\n",
      "blastout/AsnicarF_2017/AsnicarF_2017__MV_MIM2_t1M14_Bt.txt no hits\n",
      "blastout/AsnicarF_2017/AsnicarF_2017__MV_MIM3_t1M14_Bt.txt no hits\n",
      "blastout/AsnicarF_2017/AsnicarF_2017__MV_MIM4_t2F15_Bt.txt no hits\n",
      "blastout/AsnicarF_2017/AsnicarF_2017__MV_MIM5_t2M14_Bt.txt no hits\n",
      "85 gyrb hits in dataset AsnicarF_2017\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_blast(blast_results_file):\n",
    "    \"\"\"\n",
    "    convert raw blast results in a table, selecting only one (the best) hit per metagenomic read\n",
    "    this blastn table is used by future scripts (prep_df and ) to find the metagenomic fasta, determine the read overlap, \n",
    "    \"\"\"\n",
    "    #this clause checks to see whether the blastn search of a given metagenomic sample fasta returned any hits\n",
    "    try:\n",
    "        blast_results=pd.read_csv(blast_results_file, sep='\\t', comment='#',header=None)     \n",
    "    except:\n",
    "        print(blast_results_file,'no hits')\n",
    "        return\n",
    "    \n",
    "    qlen_cutoff = .8 #metagenomic reads must have 80% alignment length\n",
    "    blast_results=pd.read_csv(blast_results_file, sep='\\t', comment='#',header=None)\n",
    "    blast_results.columns=['ASV','sacc','pident','length','qlen','qstart','qend','sstart','send','evalue','sseq']   \n",
    "    blast_results=blast_results[blast_results['length']/blast_results['qlen'] >= qlen_cutoff]\n",
    "    blast_results=blast_results.sort_values('evalue')  \n",
    "    blast_results=blast_results.groupby('sacc').head(1) #get top hit\n",
    "\n",
    "    sample_name=blast_results_file.split('/')[-1].split('_Bt.txt')[0]\n",
    "    blast_results['sample_name']=sample_name\n",
    "    folder=blast_results_file.split('/')[1]\n",
    "    blast_results['folder']=folder\n",
    "    outfile = blast_results_file.replace('blastout','parse_blast')\n",
    "    blast_results.to_csv(outfile,sep='\\t',index=False)\n",
    "    return(blast_results)\n",
    "\n",
    "\n",
    "def prep_df(df):  \n",
    "    \"\"\"\n",
    "    blastn may return a hit that is the full length of the ASV. So one can't just take the subject sequence\n",
    "    This scripts determine whether the blast hit spans the full length of the ASV. If it doesn't the read is labeled as long or short\n",
    "    Long hit means that the ASV had gaps in the alignment, Short hit means that the alignment score fell at the ends of the read\n",
    "    Additionally, hits be need to reverse complemented\n",
    "    \n",
    "    This script determine the metagenomic reads starting position based on where the subject/metagenomic reads aligned with the \n",
    "    ASV/query reads. i.e. if ASV start = 3 and the metagenomic reads start is 250000, the adjusted metagenomic start is  250000- 3. \n",
    "    This is complicated slightly if the read is in the rc orientation, then the ASV start is added to the metagenomic start.\n",
    "    \n",
    "    The final step of this script is to return upstream and downsteam positions that can be used to excise the hit from the metagenomic fasta \n",
    "    regardless of read orientation.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['overlap'] = np.where(df['qlen']==df['length'], 'exact', #or if not exact must be long or short\n",
    "                             np.where(df['qlen']<df['length'], 'long','short'))\n",
    "    df['orientation'] = np.where(df['send']>df['sstart'], 'in_frame', 'reverse_complement')\n",
    "    df['sstart_adj'] = np.where(df['orientation']=='in_frame',\n",
    "                                             df['sstart'] - df['qstart'],\n",
    "                                             df['sstart'] + df['qstart'] - 1)\n",
    "    df['send_adj'] = np.where(df['orientation']=='in_frame',\n",
    "                                             df['sstart_adj'] + df['qlen'],\n",
    "                                             df['sstart_adj'] - df['qlen'])\n",
    "    df['upstream'] = df.apply(lambda row:\n",
    "                                  str(min([int(row['sstart_adj']),int(row['send_adj'])])),\n",
    "                                  axis=1)\n",
    "    df['downstream'] = df.apply(lambda row:\n",
    "                                  str(max([int(row['sstart_adj']),int(row['send_adj'])])),\n",
    "                                  axis=1)\n",
    "    return(df)\n",
    "\n",
    "def fetch_sseq_adj(row):\n",
    "    \"\"\"\n",
    "    This script takes in a row of the blastn hit table after its been modified by prep_df.\n",
    "    It determines whether blastn hit was full-length or whether is was shorter or longer than the ASV seqs\n",
    "    It checks to see whether the adjusted metagenomic read start is still in bounds. For instance, if a hit \n",
    "    has a metagenomic read start of 10 but a query/ASV start position of 30 this would be out of bounds, indicated\n",
    "    by a negative upstream value.\n",
    "    \"\"\"\n",
    "    downstream = int(row['downstream'])\n",
    "    upstream = int(row['upstream'])\n",
    "    \n",
    "    if row['length']==row['qlen']: #exact seq match\n",
    "        return(pd.Series(['NA',row['sseq']]))\n",
    "    elif upstream < 0:\n",
    "        return(pd.Series(['NA','outOfBounds']))\n",
    "    else:\n",
    "        #go fishing \n",
    "        try:\n",
    "            fasta_path = row['folder']+'/'+row['sample_name']+'.fa'\n",
    "            seq_dict = SeqIO.to_dict(SeqIO.parse('data/'+fasta_path,'fasta'))\n",
    "            contig = seq_dict[row['sacc']].seq\n",
    "            len_cont = len(str(contig))\n",
    "            if downstream >len_cont: #means contig is too short, hit goes over end\n",
    "                return(pd.Series([len_cont,'outOfBounds']))\n",
    "            else:\n",
    "                sseq_adj = str(seq_dict[row['sacc']].seq)[upstream:downstream]\n",
    "                if row['orientation']=='reverse_complement':\n",
    "                    sseq_adj = Seq.reverse_complement(sseq_adj)     \n",
    "                return(pd.Series([len_cont,sseq_adj]))\n",
    "        except:\n",
    "            return('NA','contig_not_found')   \n",
    "    \n",
    "def filter_gyrb_reads(folder):     \n",
    "    \"\"\"\n",
    "    the big kahuna. It returns a table with all the gyrb hits for all sample fastas within a folder.\n",
    "    \n",
    "    this script puts it all together. Given is a dataset folder that needs to have gyrb reads exacted\n",
    "    First, all fastas within the folder are listed, then one-by-one fastas are made into blastdbs and searched with the \n",
    "    ASV.fasta. Results from on the samples within a folder are parsed (parse_blast function) concatenated into one dataframe.\n",
    "    To determine which reads are full length and/or reverse complement the dataframe is processed the with prep_df function.\n",
    "    fetch_sseq_adj then uses the info to identify adjusted reads that can be pulled from the metagenomic fasta or \n",
    "    whether these reads are out of bounds.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    fastas = [f for f in os.listdir('data/'+folder) if f.endswith('.fa')]\n",
    "    print(fastas)\n",
    "    for fasta in fastas:\n",
    "        fasta_name = fasta.split('.fa')[0]\n",
    "        #os.system(f'makeblastdb -in data/{folder}/{fasta} -dbtype nucl')\n",
    "        os.system(f'blastn -query {ASV_fasta} -db data/{folder}/{fasta}  \\\n",
    "                    -outfmt \"7 qseqid sseqid pident length qlen qstart qend sstart send evalue sseq\" \\\n",
    "                    -out blastout/{folder}/{fasta_name}_Bt.txt')\n",
    "    \n",
    "    all_files = glob.glob(f\"blastout/{folder}/*.txt\")   \n",
    "    df_from_each_file = [parse_blast(f) for f in all_files]\n",
    "    df = pd.concat(df_from_each_file, ignore_index=True)   \n",
    "    \n",
    "    df = prep_df(df)\n",
    "    df.to_csv(folder+'.txt',sep='\\t',index=False)\n",
    "    df[['contig_len','sseq_adj']] = df.apply(lambda row: fetch_sseq_adj(row),axis=1)\n",
    "    df['sseq_same']= df['sseq']==df['sseq_adj']\n",
    "    df['sseq_adj_len'] = df['sseq_adj'].apply(lambda x: len(str(x)))\n",
    "    df = df[['ASV','sacc','sample_name','folder','pident','sseq_adj_len','evalue','overlap','orientation','qstart','qend','sstart','send','sstart_adj','send_adj','upstream','downstream','sseq_same','sseq','sseq_adj']]\n",
    "    print(len(df),'gyrb hits in dataset',folder)\n",
    "    df.to_csv('sseq_adj/'+folder+'.txt',sep='\\t',index=False)\n",
    "    \n",
    "#filter_gyrb_reads('AsnicarF_2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter metagenomic fastas for gyrb hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "datasets already filtered\n",
      "['', 'AsnicarF_2017', 'BackhedF_2015', 'Bengtsson-PalmeJ_2015', 'BritoIL_2016', 'Castro-NallarE_2015', 'ChengpingW_2017', 'ChngKR_2016', 'CM_cf', 'CM_madagascar', 'CM_periimplantitis', 'CosteaPI_2017', 'DavidLA_2015', 'FengQ_2015', 'FerrettiP_2018', 'GeversD_2014', 'HanniganGD_2017', 'HeQ_2017', 'HMP_2012', 'IjazUZ_2017', 'KarlssonFH_2013', 'KosticAD_2015', 'LeChatelierE_2013', 'LiJ_2014', 'LiJ_2017', 'LiSS_2016', 'LiuW_2016', 'LomanNJ_2013', 'LoombaR_2017', 'LouisS_2016', 'NielsenHB_2014', 'Obregon-TitoAJ_2015', 'OhJ_2014', 'OlmMR_2017', 'QinJ_2012', 'QinN_2014', 'RampelliS_2015', 'RaymondF_2016', 'SchirmerM_2016', 'SmitsSA_2017', 'VatanenT_2016', 'VincentC_2016', 'VogtmannE_2016', 'WenC_2017', 'XieH_2016', 'YuJ_2015', 'ZeeviD_2015', 'ZellerG_2014']\n",
      "\n",
      "datasets left to filter\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "#list datasets that have already been filtered\n",
    "datasets_complete = [f.split('.')[0] for f in os.listdir('sseq_adj/')]\n",
    "print('\\ndatasets already filtered')\n",
    "print(datasets_complete)\n",
    "\n",
    "datasets_to_do = set(datasets_downloaded) - set(datasets_complete)\n",
    "print('\\ndatasets left to filter')\n",
    "print(datasets_to_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep for parallel processing\n",
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())\n",
    "\n",
    "#filter out gyrb seqs\n",
    "pool = mp.Pool(mp.cpu_count())  \n",
    "results = pool.map_async(filter_gyrb_reads, [folder for folder in datasets_to_do])\n",
    "pool.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge gyrb hits across datasets and add sample metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are there any samples that aren't in the metadata file?\n",
      "set()\n",
      "how many samples returned gyrb hits?\n",
      "7300\n",
      "               ASV                               sacc  \\\n",
      "0  GCF_000012825.1  NODE_219_length_15742_cov_58.1685   \n",
      "1  GCF_000012825.1  NODE_31_length_121814_cov_92.2338   \n",
      "2  GCF_000265365.1    NODE_554_length_8127_cov_3.2111   \n",
      "3  GCF_000265365.1   NODE_49_length_87972_cov_16.9398   \n",
      "4  GCF_000156075.1  NODE_5293_length_3058_cov_4.31402   \n",
      "\n",
      "                    sample_name         folder  pident  sseq_adj_len  \\\n",
      "0  AsnicarF_2017__MV_FEI4_t1Q14  AsnicarF_2017   100.0           250   \n",
      "1  AsnicarF_2017__MV_FEI4_t2Q15  AsnicarF_2017   100.0           250   \n",
      "2  AsnicarF_2017__MV_FEI5_t1Q14  AsnicarF_2017    99.6           250   \n",
      "3  AsnicarF_2017__MV_FEI5_t2Q14  AsnicarF_2017    99.6           250   \n",
      "4  AsnicarF_2017__MV_FEI5_t3Q15  AsnicarF_2017   100.0           250   \n",
      "\n",
      "          evalue overlap orientation  qstart  ...  Body Site  Body Subsite  \\\n",
      "0  2.060000e-130   exact    in_frame       1  ...      Stool           NaN   \n",
      "1  5.360000e-130   exact    in_frame       1  ...      Stool           NaN   \n",
      "2  2.110000e-128   exact    in_frame       1  ...      Stool           NaN   \n",
      "3  3.010000e-128   exact    in_frame       1  ...      Stool           NaN   \n",
      "4  1.040000e-129   exact    in_frame       1  ...      Stool           NaN   \n",
      "\n",
      "   Age (years)  Age Category  Infant Age (days)  Westernized  Country  \\\n",
      "0            1       Newborn                NaN          Yes      ITA   \n",
      "1            1       Newborn                NaN          Yes      ITA   \n",
      "2            1       Newborn                NaN          Yes      ITA   \n",
      "3            1       Newborn                NaN          Yes      ITA   \n",
      "4            1       Newborn                NaN          Yes      ITA   \n",
      "\n",
      "   Westernized_edit            sample_group count  \n",
      "0           western  AsnicarF_2017__western     1  \n",
      "1           western  AsnicarF_2017__western     1  \n",
      "2           western  AsnicarF_2017__western     1  \n",
      "3           western  AsnicarF_2017__western     1  \n",
      "4           western  AsnicarF_2017__western     1  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "how many unique gyrb seqs were filtered out of metagenomic samples?\n",
      "7124\n",
      "gives the number of samples each read is found in by percentile\n",
      "0.10      1.00\n",
      "0.50      1.00\n",
      "0.70      2.00\n",
      "0.80      3.00\n",
      "0.90      7.00\n",
      "0.95     17.00\n",
      "0.99    143.31\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#merge results from complete datasets\n",
    "dataset_gyrb_hits =  glob.glob(f\"sseq_adj/*.txt\")   \n",
    "df_from_each_file = [pd.read_csv(f,sep='\\t') for f in dataset_gyrb_hits]\n",
    "datasets_merged = pd.concat(df_from_each_file, ignore_index=True)  \n",
    "datasets_merged.to_csv('final_outputs/gyrb_hits_all_samples.txt',sep='\\t',index=False)\n",
    "\n",
    "#filter out reads that are not full length or have ambig character\n",
    "datasets_merged = datasets_merged[datasets_merged['sseq_adj']!='outOfBounds']\n",
    "datasets_merged = datasets_merged[~datasets_merged['sseq_adj'].str.contains('-')]\n",
    "\n",
    "#print(datasets_merged.head())\n",
    "\n",
    "#load metadata for samples\n",
    "metadata = pd.read_csv(metagenomic_samples_file,sep='\\t')\n",
    "#print(metadata.head())\n",
    "metadata['sample_name'] = metadata['Study']+'__'+metadata['Sample ID']\n",
    "\n",
    "print(\"are there any samples that aren't in the metadata file?\")\n",
    "print(set(datasets_merged['sample_name'])-set(metadata['sample_name']))\n",
    "\n",
    "print(\"how many samples returned gyrb hits?\")\n",
    "print(len(set(datasets_merged['sample_name'])))\n",
    "\n",
    "df = datasets_merged.merge(metadata,on='sample_name')\n",
    "df['Westernized_edit']=df['Westernized'].apply(lambda x: x.replace('Yes','western').replace('No','nonwestern'))\n",
    "df['sample_group'] = df['folder']+'__'+df['Westernized_edit']\n",
    "df['count'] = 1 #read detect in that one sample\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df_select = df.groupby(['sample_name','sseq_adj'])['count'].sum().reset_index()\n",
    "seqtab = df_select.pivot(index='sample_name',columns='sseq_adj',values='count').fillna(0)  \n",
    "seqtab.to_csv('final_outputs/gyrb_metagenomic_seqtab.txt',sep='\\t')\n",
    "\n",
    "print('how many unique gyrb seqs were filtered out of metagenomic samples?')\n",
    "print(len(seqtab.columns))\n",
    "\n",
    "print('gives the number of samples each read is found in by percentile')\n",
    "print(seqtab.sum(axis=0).quantile([.10,.50,.70,.80,.90,.95,.99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output fasta of unqiue metagenomic reads\n",
    "use first sample as header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7124\n"
     ]
    }
   ],
   "source": [
    "df_unique = df.groupby('sseq_adj').head(1).reset_index()\n",
    "print(len(df_unique))\n",
    "with open('final_outputs/gyrb_metagenomic_unique.fasta' ,'w') as f:\n",
    "    for index,row in df_unique.iterrows():\n",
    "        header='>'+str(row['sample_name'])+'\\n'\n",
    "        seq=row['sseq_adj']+'\\n'\n",
    "        f.write(header)\n",
    "        f.write(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7124\n",
      "0.10      1.00\n",
      "0.50      1.00\n",
      "0.70      2.00\n",
      "0.80      3.00\n",
      "0.90      7.00\n",
      "0.95     17.00\n",
      "0.99    143.31\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

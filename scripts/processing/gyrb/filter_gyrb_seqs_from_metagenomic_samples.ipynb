{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script downloads metagenomic datasets useds by Pasoli et al. 2019 from their lab repository. This fastas unzipped take up roughly 2.3T. gyrb reads are filtered out using a blastn search by a fasta containing ASVs from the amplicon datasets and reads from the gyrb gtdbtk reference fasta. Reads are output into an ASV table where the columns represent the sequences and the rows represent sample names, and the values represent the number of samples that particular read was found. This enables the metagenomic data to be merged with the amplicon data. Although it should be noted that the read numbers between the two types of datasets are incomparable, and should be used to indicate presene/absence in various sample types.\n",
    "\n",
    "Inputs: list of metagenomic datasets https, Bt-ASV fasta\n",
    "\n",
    "Output: ASV table of metagenomic reads to be merged with amplicon datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio import Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of inputs/outpus\n",
    "due to the size of the data file only the results files will be permanently saved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#root_dir = '/Volumes/APE_MacPro_External_2/alex/captive_ape_microbiome'\n",
    "root_dir = '/Volumes/AHN/captive_ape_microbiome/'\n",
    "os.chdir(root_dir)\n",
    "\n",
    "analysis_dir = 'results/gyrb/processing/metagenomic_samples/'\n",
    "os.chdir(analysis_dir)\n",
    "\n",
    "metagenomic_https_file = 'metadata_http_to_dnload.txt'\n",
    "metagenomic_samples_file = 'metagenomic_metadata_all_samples.txt'\n",
    "ASV_fasta =  'Bt_ASV_gtdbtk.fasta'\n",
    "\n",
    "os.system('mkdir data') #\n",
    "os.system('mkdir sample_files')\n",
    "os.system('mkdir blastout')\n",
    "os.system('mkdir parse_blast')\n",
    "os.system('mkdir sseq_adj')\n",
    "os.system('mkdir final_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>https</th>\n",
       "      <th>filename</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dropbox.com/s/njbwzpazz4a1dy1/Asni...</td>\n",
       "      <td>AsnicarF_2017.tar.bz2</td>\n",
       "      <td>AsnicarF_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.dropbox.com/s/n6867hhmo12xnwf/Back...</td>\n",
       "      <td>BackhedF_2015.tar.bz2</td>\n",
       "      <td>BackhedF_2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dropbox.com/s/ac6cpq50pt5mqj6/Beng...</td>\n",
       "      <td>Bengtsson-PalmeJ_2015.tar.bz2</td>\n",
       "      <td>Bengtsson-PalmeJ_2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.dropbox.com/s/uj9cy9914l0cq3x/Brit...</td>\n",
       "      <td>BritoIL_2016.tar.bz2</td>\n",
       "      <td>BritoIL_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dropbox.com/s/dtxmz1apdgtf3d1/Cast...</td>\n",
       "      <td>Castro-NallarE_2015.tar.bz2</td>\n",
       "      <td>Castro-NallarE_2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               https  \\\n",
       "0  https://www.dropbox.com/s/njbwzpazz4a1dy1/Asni...   \n",
       "1  https://www.dropbox.com/s/n6867hhmo12xnwf/Back...   \n",
       "2  https://www.dropbox.com/s/ac6cpq50pt5mqj6/Beng...   \n",
       "3  https://www.dropbox.com/s/uj9cy9914l0cq3x/Brit...   \n",
       "4  https://www.dropbox.com/s/dtxmz1apdgtf3d1/Cast...   \n",
       "\n",
       "                        filename                 folder  \n",
       "0          AsnicarF_2017.tar.bz2          AsnicarF_2017  \n",
       "1          BackhedF_2015.tar.bz2          BackhedF_2015  \n",
       "2  Bengtsson-PalmeJ_2015.tar.bz2  Bengtsson-PalmeJ_2015  \n",
       "3           BritoIL_2016.tar.bz2           BritoIL_2016  \n",
       "4    Castro-NallarE_2015.tar.bz2    Castro-NallarE_2015  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate table of metagenomic datasets \n",
    "metagenomic_datasets = pd.read_csv(metagenomic_https_file,header=None)\n",
    "metagenomic_datasets.columns = ['https']\n",
    "metagenomic_datasets['filename'] = metagenomic_datasets['https'].apply(lambda x: x.split('/')[-1])\n",
    "metagenomic_datasets['folder'] = metagenomic_datasets['filename'].apply(lambda x: x.split('.')[0])\n",
    "metagenomic_datasets.head()\n",
    "metagenomic_datasets.to_csv('metadata_datasets.txt',sep='\\t',index=False)\n",
    "metagenomic_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnload_file(http): \n",
    "    \"\"\"\n",
    "    downloads and expands metagenomic dataset from Passoli repo, generates lists of sample files in the data folder\n",
    "    \"\"\"\n",
    "    filename = http.split('/')[-1]\n",
    "    folder = filename.split('.')[0]\n",
    "    print(http,filename,folder)\n",
    "    \n",
    "    os.system(f'wget {http} --directory-prefix data/')\n",
    "    os.system(f'tar -vxf data/{filename} -C data/ ')\n",
    "    os.system(f'ls data/{folder}/*.fa > sample_files/{folder}_files.txt')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dropbox.com/s/njbwzpazz4a1dy1/AsnicarF_2017.tar.bz2 AsnicarF_2017.tar.bz2 AsnicarF_2017\n"
     ]
    }
   ],
   "source": [
    "#test download for one dataset\n",
    "AsnicarF_http = metagenomic_datasets['https'].iloc[0]\n",
    "dnload_file(AsnicarF_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets already downloaded\n",
      "['AsnicarF_2017']\n",
      "\n",
      "datasets left to download\n",
      "                                                https  \\\n",
      "1   https://www.dropbox.com/s/n6867hhmo12xnwf/Back...   \n",
      "2   https://www.dropbox.com/s/ac6cpq50pt5mqj6/Beng...   \n",
      "3   https://www.dropbox.com/s/uj9cy9914l0cq3x/Brit...   \n",
      "4   https://www.dropbox.com/s/dtxmz1apdgtf3d1/Cast...   \n",
      "5   https://www.dropbox.com/s/0469lqd8m779nxe/Chen...   \n",
      "6   https://www.dropbox.com/s/whsav7tbeefgjww/Chng...   \n",
      "7   https://www.dropbox.com/s/desbb91o2pq485k/CM_c...   \n",
      "8   https://www.dropbox.com/s/plett88rfu15v7x/CM_m...   \n",
      "9   https://www.dropbox.com/s/ly8ncdsw9y3wbln/CM_p...   \n",
      "10  https://www.dropbox.com/s/qo0zudiqgelkqys/Cost...   \n",
      "11  https://www.dropbox.com/s/azfhfhfgq2tshv6/Davi...   \n",
      "12  https://www.dropbox.com/s/35i74mbblc6rivy/Feng...   \n",
      "13  https://www.dropbox.com/s/3ignf2yinb4lqp7/Ferr...   \n",
      "14  https://www.dropbox.com/s/42vc198r249t76u/Geve...   \n",
      "15  https://www.dropbox.com/s/9l9l6meb4fq7zic/Hann...   \n",
      "16  https://www.dropbox.com/s/din7jlzdk90timb/HeQ_...   \n",
      "17  https://www.dropbox.com/s/l13sis5ek52wecg/HMP_...   \n",
      "18  https://www.dropbox.com/s/pgbx0ueq0g2h286/Ijaz...   \n",
      "19  https://www.dropbox.com/s/znet4c76pckz5rc/Karl...   \n",
      "20  https://www.dropbox.com/s/i7mcj7m5o4c9kxu/Kost...   \n",
      "21  https://www.dropbox.com/s/uedzzcj72no1ze2/LeCh...   \n",
      "22  https://www.dropbox.com/s/nywdbbg49ejmzcl/LiJ_...   \n",
      "23  https://www.dropbox.com/s/wivam5bxegt36wx/LiJ_...   \n",
      "24  https://www.dropbox.com/s/l996w19jfdr0uox/LiSS...   \n",
      "25  https://www.dropbox.com/s/9we18t1cw12xg0o/LiuW...   \n",
      "26  https://www.dropbox.com/s/ql0jl00s715tpin/Loma...   \n",
      "27  https://www.dropbox.com/s/y43dhyhrthhprai/Loom...   \n",
      "28  https://www.dropbox.com/s/oq0fihi3r68wp45/Loui...   \n",
      "29  https://www.dropbox.com/s/eueu5a88gbmv2tk/Niel...   \n",
      "30  https://www.dropbox.com/s/l9m9vkmderpj3tz/Obre...   \n",
      "31  https://www.dropbox.com/s/x8vehicrc3q4alo/OhJ_...   \n",
      "32  https://www.dropbox.com/s/mmyje63uhkomk3c/OlmM...   \n",
      "33  https://www.dropbox.com/s/7sdri49mia7t4o8/QinJ...   \n",
      "34  https://www.dropbox.com/s/nrhv9qpgrx9zpq2/QinN...   \n",
      "35  https://www.dropbox.com/s/5qqtbyuufmgycp6/Ramp...   \n",
      "36  https://www.dropbox.com/s/37ln2h7mm4dmqcr/Raym...   \n",
      "37  https://www.dropbox.com/s/fodwbk30k0kinki/Schi...   \n",
      "38  https://www.dropbox.com/s/gzptqqdwwrrbn5x/Smit...   \n",
      "39  https://www.dropbox.com/s/xgkxi4ikg066prh/Vata...   \n",
      "40  https://www.dropbox.com/s/c86kwf7wro646j4/Vinc...   \n",
      "41  https://www.dropbox.com/s/9kpq5jgsmlghj9d/Vogt...   \n",
      "42  https://www.dropbox.com/s/k0vcye341lm6w4s/WenC...   \n",
      "43  https://www.dropbox.com/s/s3poffrao9p196c/YuJ_...   \n",
      "44  https://www.dropbox.com/s/o21tjgl2p3atver/Zeev...   \n",
      "45  https://www.dropbox.com/s/wofm36jatqrzty5/Zell...   \n",
      "46  https://www.dropbox.com/s/s9qu6h3ub6unbdz/XieH...   \n",
      "\n",
      "                         filename                 folder  \n",
      "1           BackhedF_2015.tar.bz2          BackhedF_2015  \n",
      "2   Bengtsson-PalmeJ_2015.tar.bz2  Bengtsson-PalmeJ_2015  \n",
      "3            BritoIL_2016.tar.bz2           BritoIL_2016  \n",
      "4     Castro-NallarE_2015.tar.bz2    Castro-NallarE_2015  \n",
      "5         ChengpingW_2017.tar.bz2        ChengpingW_2017  \n",
      "6             ChngKR_2016.tar.bz2            ChngKR_2016  \n",
      "7                   CM_cf.tar.bz2                  CM_cf  \n",
      "8           CM_madagascar.tar.bz2          CM_madagascar  \n",
      "9      CM_periimplantitis.tar.bz2     CM_periimplantitis  \n",
      "10          CosteaPI_2017.tar.bz2          CosteaPI_2017  \n",
      "11           DavidLA_2015.tar.bz2           DavidLA_2015  \n",
      "12             FengQ_2015.tar.bz2             FengQ_2015  \n",
      "13         FerrettiP_2018.tar.bz2         FerrettiP_2018  \n",
      "14           GeversD_2014.tar.bz2           GeversD_2014  \n",
      "15        HanniganGD_2017.tar.bz2        HanniganGD_2017  \n",
      "16               HeQ_2017.tar.bz2               HeQ_2017  \n",
      "17               HMP_2012.tar.bz2               HMP_2012  \n",
      "18            IjazUZ_2017.tar.bz2            IjazUZ_2017  \n",
      "19        KarlssonFH_2013.tar.bz2        KarlssonFH_2013  \n",
      "20          KosticAD_2015.tar.bz2          KosticAD_2015  \n",
      "21      LeChatelierE_2013.tar.bz2      LeChatelierE_2013  \n",
      "22               LiJ_2014.tar.bz2               LiJ_2014  \n",
      "23               LiJ_2017.tar.bz2               LiJ_2017  \n",
      "24              LiSS_2016.tar.bz2              LiSS_2016  \n",
      "25              LiuW_2016.tar.bz2              LiuW_2016  \n",
      "26           LomanNJ_2013.tar.bz2           LomanNJ_2013  \n",
      "27           LoombaR_2017.tar.bz2           LoombaR_2017  \n",
      "28            LouisS_2016.tar.bz2            LouisS_2016  \n",
      "29         NielsenHB_2014.tar.bz2         NielsenHB_2014  \n",
      "30    Obregon-TitoAJ_2015.tar.bz2    Obregon-TitoAJ_2015  \n",
      "31               OhJ_2014.tar.bz2               OhJ_2014  \n",
      "32             OlmMR_2017.tar.bz2             OlmMR_2017  \n",
      "33              QinJ_2012.tar.bz2              QinJ_2012  \n",
      "34              QinN_2014.tar.bz2              QinN_2014  \n",
      "35         RampelliS_2015.tar.bz2         RampelliS_2015  \n",
      "36          RaymondF_2016.tar.bz2          RaymondF_2016  \n",
      "37         SchirmerM_2016.tar.bz2         SchirmerM_2016  \n",
      "38           SmitsSA_2017.tar.bz2           SmitsSA_2017  \n",
      "39          VatanenT_2016.tar.bz2          VatanenT_2016  \n",
      "40          VincentC_2016.tar.bz2          VincentC_2016  \n",
      "41         VogtmannE_2016.tar.bz2         VogtmannE_2016  \n",
      "42              WenC_2017.tar.bz2              WenC_2017  \n",
      "43               YuJ_2015.tar.bz2               YuJ_2015  \n",
      "44            ZeeviD_2015.tar.bz2            ZeeviD_2015  \n",
      "45           ZellerG_2014.tar.bz2           ZellerG_2014  \n",
      "46              XieH_2016.tar.bz2              XieH_2016  \n",
      "https://www.dropbox.com/s/n6867hhmo12xnwf/BackhedF_2015.tar.bz2\n",
      "https://www.dropbox.com/s/ac6cpq50pt5mqj6/Bengtsson-PalmeJ_2015.tar.bz2\n",
      "https://www.dropbox.com/s/uj9cy9914l0cq3x/BritoIL_2016.tar.bz2\n",
      "https://www.dropbox.com/s/dtxmz1apdgtf3d1/Castro-NallarE_2015.tar.bz2\n",
      "https://www.dropbox.com/s/0469lqd8m779nxe/ChengpingW_2017.tar.bz2\n",
      "https://www.dropbox.com/s/whsav7tbeefgjww/ChngKR_2016.tar.bz2\n",
      "https://www.dropbox.com/s/desbb91o2pq485k/CM_cf.tar.bz2\n",
      "https://www.dropbox.com/s/plett88rfu15v7x/CM_madagascar.tar.bz2\n",
      "https://www.dropbox.com/s/ly8ncdsw9y3wbln/CM_periimplantitis.tar.bz2\n",
      "https://www.dropbox.com/s/qo0zudiqgelkqys/CosteaPI_2017.tar.bz2\n",
      "https://www.dropbox.com/s/azfhfhfgq2tshv6/DavidLA_2015.tar.bz2\n",
      "https://www.dropbox.com/s/35i74mbblc6rivy/FengQ_2015.tar.bz2\n",
      "https://www.dropbox.com/s/3ignf2yinb4lqp7/FerrettiP_2018.tar.bz2\n",
      "https://www.dropbox.com/s/42vc198r249t76u/GeversD_2014.tar.bz2\n",
      "https://www.dropbox.com/s/9l9l6meb4fq7zic/HanniganGD_2017.tar.bz2\n",
      "https://www.dropbox.com/s/din7jlzdk90timb/HeQ_2017.tar.bz2\n",
      "https://www.dropbox.com/s/l13sis5ek52wecg/HMP_2012.tar.bz2\n",
      "https://www.dropbox.com/s/pgbx0ueq0g2h286/IjazUZ_2017.tar.bz2\n",
      "https://www.dropbox.com/s/znet4c76pckz5rc/KarlssonFH_2013.tar.bz2\n",
      "https://www.dropbox.com/s/i7mcj7m5o4c9kxu/KosticAD_2015.tar.bz2\n",
      "https://www.dropbox.com/s/uedzzcj72no1ze2/LeChatelierE_2013.tar.bz2\n",
      "https://www.dropbox.com/s/nywdbbg49ejmzcl/LiJ_2014.tar.bz2\n",
      "https://www.dropbox.com/s/wivam5bxegt36wx/LiJ_2017.tar.bz2\n",
      "https://www.dropbox.com/s/l996w19jfdr0uox/LiSS_2016.tar.bz2\n",
      "https://www.dropbox.com/s/9we18t1cw12xg0o/LiuW_2016.tar.bz2\n",
      "https://www.dropbox.com/s/ql0jl00s715tpin/LomanNJ_2013.tar.bz2\n",
      "https://www.dropbox.com/s/y43dhyhrthhprai/LoombaR_2017.tar.bz2\n",
      "https://www.dropbox.com/s/oq0fihi3r68wp45/LouisS_2016.tar.bz2\n",
      "https://www.dropbox.com/s/eueu5a88gbmv2tk/NielsenHB_2014.tar.bz2\n",
      "https://www.dropbox.com/s/l9m9vkmderpj3tz/Obregon-TitoAJ_2015.tar.bz2\n",
      "https://www.dropbox.com/s/x8vehicrc3q4alo/OhJ_2014.tar.bz2\n",
      "https://www.dropbox.com/s/mmyje63uhkomk3c/OlmMR_2017.tar.bz2\n",
      "https://www.dropbox.com/s/7sdri49mia7t4o8/QinJ_2012.tar.bz2\n",
      "https://www.dropbox.com/s/nrhv9qpgrx9zpq2/QinN_2014.tar.bz2\n",
      "https://www.dropbox.com/s/5qqtbyuufmgycp6/RampelliS_2015.tar.bz2\n",
      "https://www.dropbox.com/s/37ln2h7mm4dmqcr/RaymondF_2016.tar.bz2\n",
      "https://www.dropbox.com/s/fodwbk30k0kinki/SchirmerM_2016.tar.bz2\n",
      "https://www.dropbox.com/s/gzptqqdwwrrbn5x/SmitsSA_2017.tar.bz2\n",
      "https://www.dropbox.com/s/xgkxi4ikg066prh/VatanenT_2016.tar.bz2\n",
      "https://www.dropbox.com/s/c86kwf7wro646j4/VincentC_2016.tar.bz2\n",
      "https://www.dropbox.com/s/9kpq5jgsmlghj9d/VogtmannE_2016.tar.bz2\n",
      "https://www.dropbox.com/s/k0vcye341lm6w4s/WenC_2017.tar.bz2\n",
      "https://www.dropbox.com/s/s3poffrao9p196c/YuJ_2015.tar.bz2\n",
      "https://www.dropbox.com/s/o21tjgl2p3atver/ZeeviD_2015.tar.bz2\n",
      "https://www.dropbox.com/s/wofm36jatqrzty5/ZellerG_2014.tar.bz2\n",
      "https://www.dropbox.com/s/s9qu6h3ub6unbdz/XieH_2016.tar.bz2\n"
     ]
    }
   ],
   "source": [
    "#list datasets already downloaded\n",
    "datasets_downloaded = [f.split('.')[0] for f in os.listdir('data/') if f.endswith('bz2')]\n",
    "print('datasets already downloaded')\n",
    "print(datasets_downloaded)\n",
    "\n",
    "#determine datasets that need to be downloads\n",
    "datasets_to_do = metagenomic_datasets[~metagenomic_datasets['folder'].isin(datasets_downloaded)]\n",
    "print('\\ndatasets left to download')\n",
    "print(datasets_to_do)\n",
    "for http in datasets_to_do['https']:\n",
    "    print(http)\n",
    "    #dnload_file(http) #uncomment to download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper scripts big to small for filtering gyrb reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AsnicarF_2017__MV_FEI1_t1Q14.fa', 'AsnicarF_2017__MV_FEI2_t1Q14.fa', 'AsnicarF_2017__MV_FEI3_t1Q14.fa', 'AsnicarF_2017__MV_FEI4_t1Q14.fa', 'AsnicarF_2017__MV_FEI4_t2Q15.fa', 'AsnicarF_2017__MV_FEI5_t1Q14.fa', 'AsnicarF_2017__MV_FEI5_t2Q14.fa', 'AsnicarF_2017__MV_FEI5_t3Q15.fa', 'AsnicarF_2017__MV_FEM1_t1Q14.fa', 'AsnicarF_2017__MV_FEM2_t1Q14.fa', 'AsnicarF_2017__MV_FEM3_t1Q14.fa', 'AsnicarF_2017__MV_FEM4_t1Q14.fa', 'AsnicarF_2017__MV_FEM4_t2Q15.fa', 'AsnicarF_2017__MV_FEM5_t1Q14.fa', 'AsnicarF_2017__MV_FEM5_t2Q14.fa', 'AsnicarF_2017__MV_FEM5_t3Q15.fa', 'AsnicarF_2017__MV_MIM2_t1M14.fa', 'AsnicarF_2017__MV_MIM3_t1M14.fa', 'AsnicarF_2017__MV_MIM4_t2F15.fa', 'AsnicarF_2017__MV_MIM5_t2M14.fa', 'AsnicarF_2017__MV_MIM5_t3F15.fa']\n",
      "blastout/AsnicarF_2017/AsnicarF_2017__MV_MIM2_t1M14_Bt.txt no hits\n",
      "blastout/AsnicarF_2017/AsnicarF_2017__MV_MIM3_t1M14_Bt.txt no hits\n",
      "blastout/AsnicarF_2017/AsnicarF_2017__MV_MIM4_t2F15_Bt.txt no hits\n",
      "blastout/AsnicarF_2017/AsnicarF_2017__MV_MIM5_t2M14_Bt.txt no hits\n",
      "85 gyrb hits in dataset AsnicarF_2017\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_blast(blast_results_file):\n",
    "    \"\"\"\n",
    "    convert raw blast results in a table, selecting only one (the best) hit per metagenomic read\n",
    "    this blastn table is used by future scripts (prep_df and ) to find the metagenomic fasta, determine the read overlap, \n",
    "    \"\"\"\n",
    "    #this clause checks to see whether the blastn search of a given metagenomic sample fasta returned any hits\n",
    "    try:\n",
    "        blast_results=pd.read_csv(blast_results_file, sep='\\t', comment='#',header=None)     \n",
    "    except:\n",
    "        print(blast_results_file,'no hits')\n",
    "        return\n",
    "    \n",
    "    qlen_cutoff = .8 #metagenomic reads must have 80% alignment length\n",
    "    blast_results=pd.read_csv(blast_results_file, sep='\\t', comment='#',header=None)\n",
    "    blast_results.columns=['ASV','sacc','pident','length','qlen','qstart','qend','sstart','send','evalue','sseq']   \n",
    "    blast_results=blast_results[blast_results['length']/blast_results['qlen'] >= qlen_cutoff]\n",
    "    blast_results=blast_results.sort_values('evalue')  \n",
    "    blast_results=blast_results.groupby('sacc').head(1) #get top hit\n",
    "\n",
    "    sample_name=blast_results_file.split('/')[-1].split('_Bt.txt')[0]\n",
    "    blast_results['sample_name']=sample_name\n",
    "    folder=blast_results_file.split('/')[1]\n",
    "    blast_results['folder']=folder\n",
    "    outfile = blast_results_file.replace('blastout','parse_blast')\n",
    "    blast_results.to_csv(outfile,sep='\\t',index=False)\n",
    "    return(blast_results)\n",
    "\n",
    "\n",
    "def prep_df(df):  \n",
    "    \"\"\"\n",
    "    blastn may return a hit that is not the full length of the ASV. So one can't just take the subject sequence\n",
    "    This scripts determine whether the blast hit spans the full length of the ASV. If it doesn't, the read is labeled as long or short\n",
    "    Long hit means that the ASV had gaps in the alignment, Short hit means that the alignment score fell at the ends of the read\n",
    "    Additionally, hits may be need to reverse complemented\n",
    "    \n",
    "    This script determine the metagenomic reads starting position based on where the subject/metagenomic reads aligned with the \n",
    "    ASV/query reads. i.e. if ASV start = 3 and the metagenomic reads start is 250000, the adjusted metagenomic start is  250000-3. \n",
    "    This is complicated slightly if the read is in the rc orientation, then the ASV start is added to the metagenomic start.\n",
    "    \n",
    "    The final step of this script is to return upstream and downsteam positions that can be used to excise the hit from the metagenomic fasta \n",
    "    regardless of read orientation.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['overlap'] = np.where(df['qlen']==df['length'], 'exact', #or if not exact must be long or short\n",
    "                             np.where(df['qlen']<df['length'], 'long','short'))\n",
    "    df['orientation'] = np.where(df['send']>df['sstart'], 'in_frame', 'reverse_complement')\n",
    "    df['sstart_adj'] = np.where(df['orientation']=='in_frame',\n",
    "                                             df['sstart'] - df['qstart'],\n",
    "                                             df['sstart'] + df['qstart'] - 1)\n",
    "    df['send_adj'] = np.where(df['orientation']=='in_frame',\n",
    "                                             df['sstart_adj'] + df['qlen'],\n",
    "                                             df['sstart_adj'] - df['qlen'])\n",
    "    df['upstream'] = df.apply(lambda row:\n",
    "                                  str(min([int(row['sstart_adj']),int(row['send_adj'])])),\n",
    "                                  axis=1)\n",
    "    df['downstream'] = df.apply(lambda row:\n",
    "                                  str(max([int(row['sstart_adj']),int(row['send_adj'])])),\n",
    "                                  axis=1)\n",
    "    return(df)\n",
    "\n",
    "def fetch_sseq_adj(row):\n",
    "    \"\"\"\n",
    "    This script takes in a row of the blastn hit table after its been modified by prep_df.\n",
    "    It determines whether blastn hit was full-length or whether is was shorter or longer than the ASV seqs\n",
    "    It checks to see whether the adjusted metagenomic read start is still in bounds. For instance, if a hit \n",
    "    has a metagenomic read start of 10 but a query/ASV start position of 30 this would be out of bounds, indicated\n",
    "    by a negative upstream value.\n",
    "    \"\"\"\n",
    "    downstream = int(row['downstream'])\n",
    "    upstream = int(row['upstream'])\n",
    "    \n",
    "    if row['length']==row['qlen']: #exact seq match\n",
    "        return(pd.Series(['NA',row['sseq']]))\n",
    "    elif upstream < 0:\n",
    "        return(pd.Series(['NA','outOfBounds']))\n",
    "    else:\n",
    "        #go fishing \n",
    "        try:\n",
    "            fasta_path = row['folder']+'/'+row['sample_name']+'.fa'\n",
    "            seq_dict = SeqIO.to_dict(SeqIO.parse('data/'+fasta_path,'fasta'))\n",
    "            contig = seq_dict[row['sacc']].seq\n",
    "            len_cont = len(str(contig))\n",
    "            if downstream >len_cont: #means contig is too short, hit goes over end\n",
    "                return(pd.Series([len_cont,'outOfBounds']))\n",
    "            else:\n",
    "                sseq_adj = str(seq_dict[row['sacc']].seq)[upstream:downstream]\n",
    "                if row['orientation']=='reverse_complement':\n",
    "                    sseq_adj = Seq.reverse_complement(sseq_adj)     \n",
    "                return(pd.Series([len_cont,sseq_adj]))\n",
    "        except:\n",
    "            return('NA','contig_not_found')   \n",
    "    \n",
    "def filter_gyrb_reads(folder):     \n",
    "    \"\"\"\n",
    "    the big kahuna. It returns a table with all the gyrb hits for all sample fastas within a folder.\n",
    "    \n",
    "    this script puts it all together. Given is a dataset folder that needs to have gyrb reads exacted\n",
    "    First, all fastas within the folder are listed, then one-by-one fastas are made into blastdbs and searched with the \n",
    "    ASV.fasta. Results from on the samples within a folder are parsed (parse_blast function) concatenated into one dataframe.\n",
    "    To determine which reads are full length and/or reverse complement the dataframe is processed the with prep_df function.\n",
    "    fetch_sseq_adj then uses the info to identify adjusted reads that can be pulled from the metagenomic fasta or \n",
    "    whether these reads are out of bounds. \n",
    "    \"\"\"\n",
    "    \n",
    "    fastas = [f for f in os.listdir('data/'+folder) if f.endswith('.fa')]\n",
    "    print(fastas)\n",
    "    for fasta in fastas:\n",
    "        fasta_name = fasta.split('.fa')[0]\n",
    "        os.system(f'makeblastdb -in data/{folder}/{fasta} -dbtype nucl')\n",
    "        os.system(f'blastn -query {ASV_fasta} -db data/{folder}/{fasta}  \\\n",
    "                    -outfmt \"7 qseqid sseqid pident length qlen qstart qend sstart send evalue sseq\" \\\n",
    "                    -out blastout/{folder}/{fasta_name}_Bt.txt')\n",
    "    \n",
    "    all_files = glob.glob(f\"blastout/{folder}/*.txt\")   \n",
    "    df_from_each_file = [parse_blast(f) for f in all_files]\n",
    "    df = pd.concat(df_from_each_file, ignore_index=True)   \n",
    "    \n",
    "    df = prep_df(df)\n",
    "    df[['contig_len','sseq_adj']] = df.apply(lambda row: fetch_sseq_adj(row),axis=1)\n",
    "    df['sseq_same']= df['sseq']==df['sseq_adj']\n",
    "    df['sseq_adj_len'] = df['sseq_adj'].apply(lambda x: len(str(x)))\n",
    "    df = df[['ASV','sacc','sample_name','folder','pident','sseq_adj_len','evalue','overlap','orientation','qstart','qend','sstart','send','sstart_adj','send_adj','upstream','downstream','sseq_same','sseq','sseq_adj']]\n",
    "    print(len(df),'gyrb hits in dataset',folder)\n",
    "    df.to_csv('sseq_adj/'+folder+'.txt',sep='\\t',index=False)\n",
    "\n",
    "#test filter gyrb\n",
    "filter_gyrb_reads('AsnicarF_2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter metagenomic fastas for gyrb hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "datasets already filtered\n",
      "['', 'AsnicarF_2017', 'BackhedF_2015', 'Bengtsson-PalmeJ_2015', 'BritoIL_2016', 'Castro-NallarE_2015', 'ChengpingW_2017', 'ChngKR_2016', 'CM_cf', 'CM_madagascar', 'CM_periimplantitis', 'CosteaPI_2017', 'DavidLA_2015', 'FengQ_2015', 'FerrettiP_2018', 'GeversD_2014', 'HanniganGD_2017', 'HeQ_2017', 'HMP_2012', 'IjazUZ_2017', 'KarlssonFH_2013', 'KosticAD_2015', 'LeChatelierE_2013', 'LiJ_2014', 'LiJ_2017', 'LiSS_2016', 'LiuW_2016', 'LomanNJ_2013', 'LoombaR_2017', 'LouisS_2016', 'NielsenHB_2014', 'Obregon-TitoAJ_2015', 'OhJ_2014', 'OlmMR_2017', 'QinJ_2012', 'QinN_2014', 'RampelliS_2015', 'RaymondF_2016', 'SchirmerM_2016', 'SmitsSA_2017', 'VatanenT_2016', 'VincentC_2016', 'VogtmannE_2016', 'WenC_2017', 'XieH_2016', 'YuJ_2015', 'ZeeviD_2015', 'ZellerG_2014']\n",
      "\n",
      "datasets left to filter\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "#list datasets that have already been filtered\n",
    "datasets_all = list(metagenomic_datasets['folder'])\n",
    "\n",
    "datasets_complete = [f.split('.')[0] for f in os.listdir('sseq_adj/')]\n",
    "print('\\ndatasets already filtered')\n",
    "print(datasets_complete)\n",
    "\n",
    "datasets_to_do = set(datasets_all) - set(datasets_complete)\n",
    "print('\\ndatasets left to filter')\n",
    "print(datasets_to_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  8\n"
     ]
    }
   ],
   "source": [
    "#prep for parallel processing\n",
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())\n",
    "\n",
    "#filter out gyrb seqs\n",
    "pool = mp.Pool(mp.cpu_count())  \n",
    "results = pool.map_async(filter_gyrb_reads, [folder for folder in datasets_to_do])\n",
    "pool.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge gyrb hits across datasets and add sample metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are there any samples that aren't in the metadata file?\n",
      "set()\n",
      "how many samples returned gyrb hits?\n",
      "7300\n",
      "how many unique gyrb seqs were filtered out of metagenomic samples?\n",
      "7124\n",
      "gives the number of samples each read is found in by percentile\n",
      "0.10      1.00\n",
      "0.50      1.00\n",
      "0.70      2.00\n",
      "0.80      3.00\n",
      "0.90      7.00\n",
      "0.95     17.00\n",
      "0.99    143.31\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#merge results from complete datasets\n",
    "dataset_gyrb_hits =  glob.glob(f\"sseq_adj/*.txt\")   \n",
    "df_from_each_file = [pd.read_csv(f,sep='\\t') for f in dataset_gyrb_hits]\n",
    "datasets_merged = pd.concat(df_from_each_file, ignore_index=True)  \n",
    "datasets_merged.to_csv('final_outputs/gyrb_hits_all_samples.txt',sep='\\t',index=False)\n",
    "\n",
    "#filter out reads that are not full length or have ambig character\n",
    "datasets_merged = datasets_merged[datasets_merged['sseq_adj']!='outOfBounds']\n",
    "datasets_merged = datasets_merged[~datasets_merged['sseq_adj'].str.contains('-')]\n",
    "\n",
    "#print(datasets_merged.head())\n",
    "\n",
    "#load metadata for samples\n",
    "metadata = pd.read_csv(metagenomic_samples_file,sep='\\t')\n",
    "#print(metadata.head())\n",
    "metadata['sample_name'] = metadata['Study']+'__'+metadata['Sample ID']\n",
    "\n",
    "print(\"are there any samples that aren't in the metadata file?\")\n",
    "print(set(datasets_merged['sample_name'])-set(metadata['sample_name']))\n",
    "\n",
    "print(\"how many samples returned gyrb hits?\")\n",
    "print(len(set(datasets_merged['sample_name'])))\n",
    "\n",
    "df = datasets_merged.merge(metadata,on='sample_name')\n",
    "df['Westernized_edit']=df['Westernized'].apply(lambda x: x.replace('Yes','western').replace('No','nonwestern'))\n",
    "df['sample_group'] = df['folder']+'__'+df['Westernized_edit']\n",
    "df['count'] = 1 #read detect in that one sample\n",
    "\n",
    "df_select = df.groupby(['sample_name','sseq_adj'])['count'].sum().reset_index()\n",
    "seqtab = df_select.pivot(index='sample_name',columns='sseq_adj',values='count').fillna(0)  \n",
    "seqtab.to_csv('final_outputs/gyrb_metagenomic_seqtab.txt',sep='\\t')\n",
    "\n",
    "print('how many unique gyrb seqs were filtered out of metagenomic samples?')\n",
    "print(len(seqtab.columns))\n",
    "\n",
    "print('gives the number of samples each read is found in by percentile')\n",
    "print(seqtab.sum(axis=0).quantile([.10,.50,.70,.80,.90,.95,.99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output fasta of unqiue metagenomic reads\n",
    "use first sample as header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7124\n"
     ]
    }
   ],
   "source": [
    "df_unique = df.groupby('sseq_adj').head(1).reset_index()\n",
    "with open('final_outputs/gyrb_metagenomic_unique.fasta' ,'w') as f:\n",
    "    for index,row in df_unique.iterrows():\n",
    "        header='>'+str(row['sample_name'])+'\\n'\n",
    "        seq=row['sseq_adj']+'\\n'\n",
    "        f.write(header)\n",
    "        f.write(seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
